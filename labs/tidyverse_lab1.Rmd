---
title: "Tidyverse lab"
author: "Ariane Ducellier"
date: "10/3/2023"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Libraries

Load the necessary libraries.

```{r libraries}
library(nycflights13)
library(tidyverse)
```

The first three exercices are done with the flights dataset from the nycflights13 package.

```{r summary_data}
summary(flights)
```

## Exercise 1

### Question 1

In a single pipeline for each condition, find all flights that meet the condition:
- Had an arrival delay of two or more hours.

```{r filter}
flights %>% filter(arr_delay >= 120)
```

- Flew to Houston (IAH or HOU).

```{r filter2}
flights %>% filter(dest %in% c("IAH", "HOU"))
```

- Were operated by United, American, or Delta.

```{r filter3}
flights %>% filter(carrier %in% c("UA", "AA", "DL"))
```

- Departed in summer (July, August, and September).

```{r filter4}
flights %>% filter(month %in% c(7, 8, 9))
```

- Arrived more than two hours late, but didn’t leave late.

```{r filter5}
flights %>% filter(arr_delay >= 120 & dep_delay <= 0)
```

- Were delayed by at least an hour, but made up more than 30 minutes in flight.

```{r filter6}
flights %>%
  mutate(delay_gained=dep_delay - arr_delay) %>%
  filter(dep_delay >= 60 & delay_gained >= 30)
```

### Question 2

Sort flights to find the flights with the longest departure delays. Find the flights that left earliest in the morning.

```{r arrange}
flights %>% arrange(desc(dep_delay))
```

```{r arrange2}
flights %>% arrange(sched_dep_time)
```

### Question 3

Sort flights to find the fastest flights.

```{r fastest}
flights %>%
  mutate(speed=distance / air_time) %>%
  arrange(desc(speed))
```

### Question 4

Was there a flight on every day of 2013?

```{r distinct}
flights %>% distinct(month, day) %>% dim()
```

### Question 5

Which flights traveled the farthest distance? Which traveled the least distance?

```{r farther}
flights %>% filter(distance == max(distance))
```


### Question 6

Does it matter what order you used filter() and arrange() if you’re using both? Why/why not? Think about the results and how much work the functions would have to do.

## Exercise 2

### Question 1

Compare dep_time, sched_dep_time, and dep_delay. How would you expect those three numbers to be related?

```{r delay}
flights %>% select(dep_time, sched_dep_time, dep_delay) %>%
  mutate(dep_hour=as.integer(floor(dep_time / 100)),
         dep_min=dep_time - dep_hour * 100,
         sched_dep_hour=as.integer(floor(sched_dep_time / 100)),
         sched_dep_min=sched_dep_time - sched_dep_hour * 100,
         next_day=(dep_time < sched_dep_time) & (dep_delay > 0)) %>%
  mutate(comp_dep_delay=ifelse(next_day,
    dep_hour * 60 + dep_min - sched_dep_hour * 60 - sched_dep_min + 1440,
    dep_hour * 60 + dep_min - sched_dep_hour * 60 - sched_dep_min)) %>%
filter(dep_delay != comp_dep_delay)
```


### Question 2

Rename air_time to air_time_min to indicate units of measurement and move it to the beginning of the data frame.

## Exercise 3

### Question 1

Which carrier has the worst average delays? Challenge: Can you disentangle the effects of bad airports versus bad carriers? Why/why not?

### Question 2

Find the flights that are most delayed upon departure from each destination.

## Question 3

How do delays vary over the course of the day? Illustrate your answer with a plot.

## Exercise 4

This exercise will make use of tidyverse functions for data transformation to extract and manipulate metadata of seismic stations in the Northern California Seismic Network.

We want to download seismic waveforms from a seismic data archive of specific earthquakes. We are not sure what seismic sensors (stations) are operating at that time. The list of stations available in the seismic networks has more than 6000, that's way too many! So we want to filter only the seismic stations that are relevant for the research.

This is the address of the website to download the data:
[NCEDC metadata](http://ncedc.org/ftp/pub/doc/NC.info/NC.channel.summary.day)

### Question 1

First, you need to load the data into a tibble. You may use the following header:

```{r header}
header = c("Station", "Network", "Channel", "Location", "Rate",
           "Start_time", "End_time", "Latitude", "Longitude",
           "Elevation", "Depth", "Dip", "Azimuth", "Instrument")
```

### Question 2

Now, we need to convert Start_time and End_time into a datetime format.

It turns out than only the following channels are relevant for the work we want to do:

- BHE, BHN, BHZ, BH1, BH2,
- EHE, EHN, EHZ, EH1, EH2,
- HHE, HHN, HHZ, HH1, HH2,
- SHE, SHN, SHZ, SH1, SH2.

That is, we want the channels that start with B, E, H or S and which second letter with an H.
            
### Question 3

Filter the dataset to keep only the rows with the channels as defined above.

The seismic data archive that we are working on starts on 2007/07/01 and ends on 2009/07/01. We are only interested in stations that started recording before 2009/07/01 and ended recording after 2007/07/01.

### Question 4

Filter the dataset to keep only stations that started recording before 2009/07/01 and ended recording after 2007/07/01.

The earthquakes we are interested in are located at latitude = 40.09N and longitude = -122.87E. 

We want to keep the stations that are located less than 100 km from the earthquakes.

### Question 5

Filter the dataset to keep only stations that are within 100 km from the earthquakes.

You may use this function to compute the distance from the station to the earthquakes using the latitude and the longitude:

```{r distance_function}
get_dists <- function(df){
  lat0 = 40.09000
  lon0 = -122.87000
  a = 6378.136
  e = 0.006694470
  dx = (pi / 180.0) * a * cos(lat0 * pi / 180.0) /
    sqrt(1.0 - e * e * sin(lat0 * pi / 180.0) * sin(lat0 * pi / 180.0))
  dy = (3.6 * pi / 648.0) * a * (1.0 - e * e) /
    ((1.0 - e * e * sin(lat0 * pi / 180.0) * sin(lat0 * pi / 180.0)) ** 1.5)
  df$x = dx * (df$Longitude - lon0)
  df$y = dy * (df$Latitude - lat0)
  df$Distance = "^"("^"(df$x, 2.0) + "^"(df$y, 2.0), 1/2)
  return (df$Distance)
}
```

For the final step, we only want to keep the columns: Station, Network, Channel, Location, Latitude, Longitude, Elevation, Depth, Start_time, End_time. We want to group the stations:

### Question 6

First, group the stations by Station, Network, Channel, Location, Latitude, Longitude, Elevation, Depth, and compute the minimum start time and the maximum end time.

### Question 7

Second, group the stations by Station, Network, Location, Latitude, Longitude, Elevation, Depth, and concatenate all the channels for a given station in a single string, separated by a comma.
